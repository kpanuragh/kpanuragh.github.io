---
title: "Consistent Hashing: The Algorithm That Stopped Me From Nuking My Entire Cache ğŸ”„ğŸ’¾"
date: "2026-02-23"
excerpt: "I added one cache node to our Redis cluster and lost 85% of all cached data in under 30 seconds. The database melted. Customers saw errors. My manager called. Consistent hashing is why that never happens to well-designed systems â€” and why it took me a full production incident to finally understand it."
tags: ["architecture", "scalability", "system-design", "caching", "distributed-systems"]
featured: true
---

# Consistent Hashing: The Algorithm That Stopped Me From Nuking My Entire Cache ğŸ”„ğŸ’¾

**Storytime:** It was 11 PM on a Wednesday. We'd just scaled our Redis cache from 3 nodes to 4 nodes to handle Black Friday traffic. Simple horizontal scaling, right?

Within 60 seconds:
- Cache hit rate: 94% â†’ **9%**
- Database CPU: 12% â†’ **96%**
- API response time: 45ms â†’ **8,400ms**
- My heart rate: **ğŸ“ˆğŸ“ˆğŸ“ˆ**

**Me:** "What theâ€” why is the database on fire?!"
**Senior Dev (from his couch, phone buzzing):** "Did you add a cache node?"
**Me:** "...yes?"
**Senior Dev:** "Did you rehash?"
**Me:** "...what's rehash?"
**Senior Dev:** *audible sigh* "I'll be online in 5 minutes."

That night I learned about consistent hashing. It's the algorithm that separates "I added a server" from "I accidentally reset the entire cache."

## The Problem With Naive Cache Distribution ğŸ¤”

When you have multiple cache nodes, you need a way to decide **which node stores which key**. The obvious solution:

```
node = hash(key) % number_of_nodes
```

Let's say you have 3 nodes and key `user:42`:

```
hash("user:42") = 1,847,293
1,847,293 % 3 = 1 â†’ Node 1
```

Simple! Beautiful! Works perfectlyâ€¦ until you add or remove a node.

**What happens when you add Node 4:**

```
Before (3 nodes):            After (4 nodes):
hash("user:42") % 3 = 1     hash("user:42") % 4 = 1  âœ… (lucky!)
hash("order:9") % 3 = 0     hash("order:9") % 4 = 1  âŒ DIFFERENT NODE
hash("cart:77") % 3 = 2     hash("cart:77") % 4 = 3  âŒ DIFFERENT NODE
hash("sess:5")  % 3 = 0     hash("sess:5")  % 4 = 0  âœ… (lucky!)
hash("prod:12") % 3 = 1     hash("prod:12") % 4 = 0  âŒ DIFFERENT NODE
```

**On average, when you add 1 node to N nodes:**

```
Keys remapped = (N / N+1) = ~75% of ALL keys!

3 â†’ 4 nodes: ~75% of keys land on wrong nodes (cache MISS)
9 â†’ 10 nodes: ~90% of keys land on wrong nodes (cache MISS)
```

**Translation:** Every time you scale, you're essentially **flushing most of your cache**. Your database absorbs all those misses. It's like adding a lane to a highway and somehow all the cars end up in a traffic jam.

This is EXACTLY what happened to us that Wednesday night.

## Enter Consistent Hashing: The Ring ğŸ’

Consistent hashing is clever. Instead of `hash(key) % N`, you imagine **both your keys AND your nodes mapped onto a ring** (a circular space from 0 to 2^32).

```
               0
           â”Œâ”€â”€â”€â”€â”€â”€â”€â”
    330Â°   â”‚       â”‚   30Â°
     â•±     â”‚  RING â”‚    â•²
    â•±      â”‚       â”‚     â•²
300Â°  â”€â”€â”€â”€â”€â”˜       â””â”€â”€â”€â”€â”€  60Â°
 â”‚                           â”‚
 â”‚  [Node C]     [Node A]    â”‚
270Â°                         90Â°
 â”‚                           â”‚
 â”‚  [Node B]                 â”‚
240Â°  â”€â”€â”€â”€â”€â”       â”Œâ”€â”€â”€â”€â”€  120Â°
    â•²      â”‚       â”‚     â•±
     â•²     â”‚       â”‚    â•±
    210Â°   â””â”€â”€â”€â”€â”€â”€â”€â”˜   150Â°
              180Â°
```

**How it works:**

1. Hash each **node name** â†’ place it on the ring at that position
2. Hash each **cache key** â†’ place it on the ring at that position
3. A key is stored on the **first node clockwise from the key's position**

```
Ring position (0 â†’ 360Â°):

Node A  â†’ hashes to position 60Â°
Node B  â†’ hashes to position 200Â°
Node C  â†’ hashes to position 320Â°

"user:42"  â†’ hashes to 45Â°  â†’ stored on Node A (first clockwise: 60Â°)
"order:9"  â†’ hashes to 130Â° â†’ stored on Node B (first clockwise: 200Â°)
"cart:77"  â†’ hashes to 280Â° â†’ stored on Node C (first clockwise: 320Â°)
"sess:5"   â†’ hashes to 350Â° â†’ stored on Node A (wrap around â†’ 60Â°)
```

Now watch what happens when you **add Node D at position 150Â°**:

```
BEFORE adding Node D:
  45Â°  â†’ Node A   âœ…
 130Â°  â†’ Node B   âœ…
 280Â°  â†’ Node C   âœ…
 350Â°  â†’ Node A   âœ…

AFTER adding Node D (at 150Â°):
  45Â°  â†’ Node A   âœ… unchanged
 130Â°  â†’ Node D   â† only keys between 60Â° and 150Â° move to D!
 280Â°  â†’ Node C   âœ… unchanged
 350Â°  â†’ Node A   âœ… unchanged
```

**Only the keys that were between Node A (60Â°) and Node D (150Â°) move.** Everything else stays put.

**The math changes dramatically:**

```
Naive hashing:  add 1 node â†’ ~75% of keys remapped ğŸ’€
Consistent:     add 1 node â†’ ~1/N keys remapped    ğŸ‰

4 â†’ 5 nodes: only ~20% of keys move (not 80%!)
9 â†’ 10 nodes: only ~10% of keys move (not ~90%!)
```

When designing our e-commerce backend, this is the difference between "we scaled smoothly" and "we got paged at 11 PM."

## Real Code: How This Looks In Practice ğŸ› ï¸

Here's a simplified consistent hash ring in Node.js:

```javascript
const crypto = require('crypto');

class ConsistentHashRing {
    constructor(nodes = [], replicas = 150) {
        this.replicas = replicas; // virtual nodes per server
        this.ring = new Map();
        this.sortedKeys = [];

        nodes.forEach(node => this.addNode(node));
    }

    hash(key) {
        return parseInt(
            crypto.createHash('md5').update(key).digest('hex').slice(0, 8),
            16
        );
    }

    addNode(node) {
        // Add 'replicas' virtual nodes for each physical node
        for (let i = 0; i < this.replicas; i++) {
            const virtualKey = this.hash(`${node}:${i}`);
            this.ring.set(virtualKey, node);
            this.sortedKeys.push(virtualKey);
        }
        this.sortedKeys.sort((a, b) => a - b);
    }

    removeNode(node) {
        for (let i = 0; i < this.replicas; i++) {
            const virtualKey = this.hash(`${node}:${i}`);
            this.ring.delete(virtualKey);
        }
        this.sortedKeys = this.sortedKeys.filter(k => this.ring.has(k));
    }

    getNode(key) {
        if (this.ring.size === 0) return null;

        const keyHash = this.hash(key);

        // Find first node clockwise (binary search)
        for (const ringKey of this.sortedKeys) {
            if (keyHash <= ringKey) {
                return this.ring.get(ringKey);
            }
        }

        // Wrap around: return first node on ring
        return this.ring.get(this.sortedKeys[0]);
    }
}

// Usage
const ring = new ConsistentHashRing(['redis-1', 'redis-2', 'redis-3']);

console.log(ring.getNode('user:42'));   // â†’ "redis-2"
console.log(ring.getNode('order:99'));  // â†’ "redis-1"
console.log(ring.getNode('cart:7'));    // â†’ "redis-3"

// Add a new node â€” only ~25% of keys remapped!
ring.addNode('redis-4');

console.log(ring.getNode('user:42'));   // â†’ "redis-2" (probably same!)
console.log(ring.getNode('order:99'));  // â†’ "redis-4" (might have moved)
```

**In production with Laravel + Redis**, this is handled automatically by Redis Cluster using consistent hashing under the hood. You don't write the ring yourself â€” but knowing how it works saves you from the Wednesday-night incident I had.

## Virtual Nodes: Why One Server Lives in Many Places ğŸ‘¥

There's a gotcha with the basic ring: **uneven distribution**.

If Node A hashes to 10Â°, Node B to 15Â°, Node C to 300Â°, then Node C handles 85% of all keys. That's not balanced.

**Solution: Virtual nodes (vnodes)** â€” each physical server maps to **multiple points on the ring**:

```
Physical servers: redis-1, redis-2, redis-3

Virtual nodes (replicas=3 for simplicity):
  redis-1:0 â†’ 45Â°    redis-1:1 â†’ 140Â°   redis-1:2 â†’ 290Â°
  redis-2:0 â†’ 80Â°    redis-2:1 â†’ 210Â°   redis-2:2 â†’ 320Â°
  redis-3:0 â†’ 20Â°    redis-3:1 â†’ 170Â°   redis-3:2 â†’ 255Â°

Ring (sorted):
  20Â°  â†’ redis-3    â† actually redis-3:0
  45Â°  â†’ redis-1    â† actually redis-1:0
  80Â°  â†’ redis-2    â† actually redis-2:0
 140Â°  â†’ redis-1    â† actually redis-1:1
 170Â°  â†’ redis-3    â† actually redis-3:1
 210Â°  â†’ redis-2    â† actually redis-2:1
 255Â°  â†’ redis-3    â† actually redis-3:2
 290Â°  â†’ redis-1    â† actually redis-1:2
 320Â°  â†’ redis-2    â† actually redis-2:2
```

Now each server is interspersed across the ring. Keys are distributed much more evenly.

**In production:** Redis Cluster uses 16,384 hash slots distributed across nodes â€” effectively a form of consistent hashing with virtual slots. Cassandra uses 256 vnodes per node by default.

## Where You're Already Using This (Without Knowing It) ğŸ•µï¸

As a Technical Lead, I've found consistent hashing hiding in places I didn't expect:

**1. Redis Cluster**
```
# Redis automatically shards keys across cluster nodes
# CLUSTER KEYSLOT shows which slot (0-16383) a key maps to
CLUSTER KEYSLOT "user:42"   â†’ 1847
# Redis then maps slot 1847 to a specific node
```

**2. AWS DynamoDB**
DynamoDB partitions use consistent hashing internally. When you add provisioned throughput, new partitions are added and only the affected key ranges move. This is why DynamoDB can scale without downtime.

**3. Nginx/HAProxy upstream hashing**
```nginx
upstream backend {
    hash $request_uri consistent;  # <-- consistent hashing!
    server app1.example.com;
    server app2.example.com;
    server app3.example.com;
}
# Same URL always routes to same server (useful for page caches!)
```

**4. Apache Cassandra**
```
# Cassandra uses a token ring (consistent hashing variant)
# Data distributes based on partition key hash
# Add a node? Only neighboring token ranges are affected!
```

**When designing our e-commerce backend**, I used consistent hashing explicitly in our API gateway for sticky routing â€” sending the same customer's requests to the same app server instance to maximize local in-memory cache hits.

## The Trade-Offs (Nothing Is Free) âš–ï¸

**Advantages:**
- Adding/removing nodes only remaps `~1/N` of keys
- Works well with dynamic node counts
- No central coordinator needed
- Scales to thousands of nodes elegantly

**Disadvantages:**

```javascript
// 1. Hot spots still possible without enough virtual nodes
// If two nodes are close together on the ring, one carries more load
// Solution: use 150+ virtual nodes (Redis Cluster uses 16,384 slots)

// 2. Replication complexity
// "Store on the next 3 nodes clockwise" â€” works but adds complexity
// DynamoDB, Cassandra both handle this for you

// 3. Not great for tiny node counts
// With 2 nodes, consistent hashing over naive hashing barely helps
// Shines at 5+ nodes

// 4. Initial distribution can be uneven
// Solved by virtual nodes, but adds memory overhead for the ring structure
```

**When NOT to use it:**
- You have 2-3 cache nodes and never change them â†’ simple `mod N` is fine
- You're using a managed service (Redis Cluster, DynamoDB, Elasticache) â†’ it's handled for you
- You need strong consistency â†’ consistent hashing is an **availability** optimization

## Common Mistakes I Made (And You Can Avoid) âŒ

**Mistake 1: Adding nodes without warmup**

```javascript
// BAD: Add all 3 new nodes at once (massive redistribution)
ring.addNode('redis-4');
ring.addNode('redis-5');
ring.addNode('redis-6');
// Cache hit rate drops immediately - DB gets hammered

// BETTER: Add one node, let cache warm up, then add next
ring.addNode('redis-4');
// Wait 30 minutes for cache to warm on new node
ring.addNode('redis-5');
// Wait 30 minutes...
ring.addNode('redis-6');
```

**Mistake 2: Forgetting replication**

```javascript
// Naive: store on exactly 1 node
const node = ring.getNode(key);
redis.set(node, key, value);

// Reality: that node can die
// Better: replicate to next N nodes on the ring
const nodes = ring.getReplicaNodes(key, 3); // next 3 nodes
nodes.forEach(node => redis.set(node, key, value)); // write to all 3
// Read from first responsive node
```

**Mistake 3: Not testing node removal**

A scalability lesson that cost us a full incident: we'd tested adding nodes but never tested *removing* one. When we decommissioned a node, we forgot to migrate its keys first. Cache miss spike, same story as adding a node.

```bash
# Before removing a node:
# 1. Mark it as draining (don't route new writes)
# 2. Wait for TTLs to expire OR migrate keys
# 3. Then remove from ring
```

## TL;DR ğŸ¯

**The 30-second version:**

```
Naive hashing (hash(key) % N):
  Add 1 node â†’ ~75% of cache GONE ğŸ’€

Consistent hashing (hash(key) on a ring):
  Add 1 node â†’ ~1/N of cache affected ğŸ‰

How it works:
  1. Place nodes AND keys on an imaginary circle
  2. Each key goes to the first node clockwise from it
  3. Adding a node only steals keys from its neighbors

Where you already use it:
  Redis Cluster, DynamoDB, Cassandra, Nginx upstream hashing

When to care:
  âœ… Dynamic cache clusters (scaling up/down)
  âœ… Building your own distributed cache/DB
  âœ… Load balancing with session affinity
  âœ… Understanding why Cassandra doesn't lose data when you add a node
```

As a Technical Lead, I've learned: you don't need to build consistent hashing yourself in most cases â€” Redis Cluster, DynamoDB, and Cassandra do it for you. But the day you scale your cache under pressure and need to understand *why* 80% of your keys are suddenly on the wrong node, you'll be glad you know this.

My 11 PM incident? We rolled back to 3 nodes, let the cache warm overnight, then migrated to Redis Cluster (which handles this automatically). The second scale-out was boring. Boring is good. ğŸ™

---

**Hit a scaling wall?** Connect with me on [LinkedIn](https://www.linkedin.com/in/anuraghkp) â€” I've been in the 11 PM incident Slack call more times than I'd like to admit.

**Curious about my e-commerce backend setup?** Check out my [GitHub](https://github.com/kpanuragh) for architecture patterns and production lessons.

*Build smart, scale boring.* ğŸ”„âœ¨

---

**P.S.** If you're still using `hash(key) % N` for a distributed cache with more than 3 nodes, please read this post twice. I'm begging you. Your future on-call self will thank you. â¤ï¸
